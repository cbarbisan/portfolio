{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your name:\n",
    "\n",
    "<pre>Craig Barbisan</pre>\n",
    "\n",
    "### Collaborators:\n",
    "\n",
    "Solutions sourced/adapted from: https://github.com/ageron/handson-ml/blob/master/09_up_and_running_with_tensorflow.ipynb</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(123)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. When is a variable initialized? When is it destroyed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Variables are initialized when you call their initializer. Variables are destroyed when the session ends.\n",
    "\n",
    "In a distributed environment, variables are held in containers on the cluster and do not get destroyed when the session ends. To destroy a variable in this scenario, you need to clear its container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What is the difference between a placeholder and a variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Placeholders hold information about the type and shape of the tensor they represent. They have no value. They are used to feed training or test data to TensorFlow during the execution phase.\n",
    "\n",
    "A variable is an operation that holds a value. If you run the variable, it returns that value. Variables need to be initialized before they can be run. A variable's value can be changed, and it keeps the same value upon successive runs of the graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. How many times does reverse-mode autodiff need to traverse the graph in order to compute the gradients of the cost function with regards to 10 variables? What about forward-mode autodiff? And symbolic differentiation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Reverse-mode autodiff needs to traverse the graph 2 times to compute the gradients of the cost function with regards to 10 variables.\n",
    "\n",
    "Forward-mode autodiff needs to traverse the graph 10 times to compute the gradients of the cost function with regards to 10 variables.\n",
    "\n",
    "Symbolic differentiation doesn't traverse the graph to compute gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Implement Logistic Regression with Mini-batch Gradient Descent using TensorFlow. Train it and evaluate it on the moons dataset (introduced in Chapter 5). Try adding all the bells and whistles:\n",
    "\n",
    "- Define the graph within a logistic_regression() function that can be reused easily.\n",
    "\n",
    "- Save checkpoints using a Saver at regular intervals during training, and save the final model at the end of training.\n",
    "\n",
    "- Restore the last checkpoint upon startup if training was interrupted.\n",
    "\n",
    "- Define the graph using name scopes so the graph looks good in TensorBoard.\n",
    "\n",
    "- Add summaries to visualize the learning curves in TensorBoard.\n",
    "\n",
    "- Try tweaking some hyperparameters such as the learning rate or the mini-batch size and look at the shape of the learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# constructs the graph for the logistic regression equation\n",
    "def logistic_regression(X, y, initializer=None, seed=42, learning_rate=0.01):\n",
    "    n_inputs_including_bias = int(X.get_shape()[1])\n",
    "    with tf.name_scope(\"logistic_regression\"):\n",
    "        with tf.name_scope(\"model\"):\n",
    "            if initializer is None:\n",
    "                initializer = tf.random_uniform([n_inputs_including_bias, 1], -1.0, 1.0, seed=seed)\n",
    "            theta = tf.Variable(initializer, name=\"theta\")\n",
    "            logits = tf.matmul(X, theta, name=\"logits\")\n",
    "            y_proba = tf.sigmoid(logits)\n",
    "        with tf.name_scope(\"train\"):\n",
    "            loss = tf.losses.log_loss(y, y_proba, scope=\"loss\")\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "            training_op = optimizer.minimize(loss)\n",
    "            loss_summary = tf.summary.scalar('log_loss', loss)\n",
    "        with tf.name_scope(\"init\"):\n",
    "            init = tf.global_variables_initializer()\n",
    "        with tf.name_scope(\"save\"):\n",
    "            saver = tf.train.Saver()\n",
    "    return y_proba, loss, training_op, loss_summary, init, saver\n",
    "\n",
    "\n",
    "def random_batch(X_train, y_train, batch_size):\n",
    "    rnd_indices = np.random.randint(0, len(X_train), batch_size)\n",
    "    X_batch = X_train[rnd_indices]\n",
    "    y_batch = y_train[rnd_indices]\n",
    "    return X_batch, y_batch\n",
    "\n",
    "def log_dir(prefix=\"\"):\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    if prefix:\n",
    "        prefix += \"-\"\n",
    "    name = prefix + \"run-\" + now\n",
    "    return \"{}/{}/\".format(root_logdir, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\craig\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\craig\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:514: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch: 0 \tLoss: 0.9290874\n",
      "Epoch: 500 \tLoss: 0.2795907\n",
      "Epoch: 1000 \tLoss: 0.27295217\n",
      "Epoch: 1500 \tLoss: 0.2734188\n",
      "Epoch: 2000 \tLoss: 0.27229783\n",
      "Epoch: 2500 \tLoss: 0.2730588\n",
      "Epoch: 3000 \tLoss: 0.27329552\n",
      "Epoch: 3500 \tLoss: 0.2730586\n",
      "Epoch: 4000 \tLoss: 0.27253303\n",
      "Epoch: 4500 \tLoss: 0.27305058\n",
      "Epoch: 5000 \tLoss: 0.2732856\n",
      "Epoch: 5500 \tLoss: 0.27387607\n",
      "Epoch: 6000 \tLoss: 0.27318084\n",
      "Epoch: 6500 \tLoss: 0.27417946\n",
      "Epoch: 7000 \tLoss: 0.2733306\n",
      "Epoch: 7500 \tLoss: 0.27319005\n",
      "Epoch: 8000 \tLoss: 0.27357602\n",
      "Epoch: 8500 \tLoss: 0.27382684\n",
      "Epoch: 9000 \tLoss: 0.2734192\n",
      "Epoch: 9500 \tLoss: 0.2732019\n",
      "Epoch: 10000 \tLoss: 0.27364567\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "import os\n",
    "\n",
    "m = 1000\n",
    "X_moons, y_moons = make_moons(m, noise=0.1)\n",
    "\n",
    "# add an extra bias feature\n",
    "X_moons_with_bias = np.c_[np.ones((m, 1)), X_moons]\n",
    "\n",
    "# reshape y_train to make it a column vector\n",
    "y_moons_column_vector = y_moons.reshape(-1, 1)\n",
    "\n",
    "# split the data into training and test\n",
    "test_ratio = 0.2\n",
    "test_size = int(m * test_ratio)\n",
    "X_train = X_moons_with_bias[:-test_size]\n",
    "X_test = X_moons_with_bias[-test_size:]\n",
    "y_train = y_moons_column_vector[:-test_size]\n",
    "y_test = y_moons_column_vector[-test_size:]\n",
    "\n",
    "# reset the default graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 2\n",
    "logdir = log_dir(\"logreg\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "y_proba, loss, training_op, loss_summary, init, saver = logistic_regression(X, y)\n",
    "\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "n_epochs = 10001\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "checkpoint_path = \"/tmp/my_logreg_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./my_logreg_model\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        # if the checkpoint file exists, restore the model and load the epoch number\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = random_batch(X_train, y_train, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, summary_str = sess.run([loss, loss_summary], feed_dict={X: X_test, y: y_test})\n",
    "        file_writer.add_summary(summary_str, epoch)\n",
    "        if epoch % 500 == 0:\n",
    "            print(\"Epoch:\", epoch, \"\\tLoss:\", loss_val)\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "\n",
    "    saver.save(sess, final_model_path)\n",
    "    y_proba_val = y_proba.eval(feed_dict={X: X_test, y: y_test})\n",
    "    os.remove(checkpoint_epoch_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Similar to the linear regression implementation in class, write a lasso regression implementation. Use the same dataset, and choose a value for the penalty $\\alpha$:\n",
    "\n",
    "Using a Saver at regular intervals during training, and save the final model at the end of training.\n",
    "\n",
    "Restore the last checkpoint upon startup if training was interrupted.\n",
    "\n",
    "Define the graph using name scopes so the graph looks good in TensorBoard.\n",
    "\n",
    "Add summaries to visualize the learning curves in TensorBoard.\n",
    "\n",
    "Try tweaking some hyperparameters such as the learning rate or the mini-batch size and look at the shape of the learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Cost = [[17.993376]\n",
      " [18.007679]\n",
      " [17.953821]\n",
      " [18.023912]\n",
      " [17.932095]\n",
      " [17.938606]\n",
      " [18.142143]\n",
      " [18.159555]\n",
      " [17.875874]]\n",
      "Epoch 100 Cost = [[10.381736]\n",
      " [10.369181]\n",
      " [10.304965]\n",
      " [10.380938]\n",
      " [10.28957 ]\n",
      " [10.298252]\n",
      " [10.467155]\n",
      " [10.452883]\n",
      " [10.270487]]\n",
      "Epoch 200 Cost = [[6.466881 ]\n",
      " [6.4313293]\n",
      " [6.362198 ]\n",
      " [6.4393077]\n",
      " [6.3486576]\n",
      " [6.358741 ]\n",
      " [6.5001454]\n",
      " [6.4682975]\n",
      " [6.351897 ]]\n",
      "Epoch 300 Cost = [[4.3335385]\n",
      " [4.278538 ]\n",
      " [4.2077947]\n",
      " [4.2831492]\n",
      " [4.193444 ]\n",
      " [4.2047086]\n",
      " [4.3238735]\n",
      " [4.283011 ]\n",
      " [4.2096534]]\n",
      "Epoch 400 Cost = [[3.1024516]\n",
      " [3.0311635]\n",
      " [2.9607437]\n",
      " [3.0326743]\n",
      " [2.9441018]\n",
      " [2.9565592]\n",
      " [3.0574539]\n",
      " [3.012652 ]\n",
      " [2.9673905]]\n",
      "Epoch 500 Cost = [[2.354402 ]\n",
      " [2.2695804]\n",
      " [2.2005563]\n",
      " [2.2681901]\n",
      " [2.1809044]\n",
      " [2.1946325]\n",
      " [2.280323 ]\n",
      " [2.2344854]\n",
      " [2.2075741]]\n",
      "Epoch 600 Cost = [[1.8799244]\n",
      " [1.783933 ]\n",
      " [1.7168378]\n",
      " [1.7798089]\n",
      " [1.6939337]\n",
      " [1.7090003]\n",
      " [1.7819062]\n",
      " [1.7366002]\n",
      " [1.7216749]]\n",
      "Epoch 700 Cost = [[1.5686082]\n",
      " [1.4634519]\n",
      " [1.3984885]\n",
      " [1.4567457]\n",
      " [1.3723792]\n",
      " [1.3888113]\n",
      " [1.4508797]\n",
      " [1.4068619]\n",
      " [1.3997613]]\n",
      "Epoch 800 Cost = [[1.3589408]\n",
      " [1.2463042]\n",
      " [1.1834801]\n",
      " [1.2371591]\n",
      " [1.154378 ]\n",
      " [1.1721557]\n",
      " [1.2249793]\n",
      " [1.1825273]\n",
      " [1.1805265]]\n",
      "Epoch 900 Cost = [[1.2148149]\n",
      " [1.0961037]\n",
      " [1.0353153]\n",
      " [1.0846555]\n",
      " [1.003519 ]\n",
      " [1.0225801]\n",
      " [1.0674796]\n",
      " [1.0265982]\n",
      " [1.0279105]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "alpha = 0.1\n",
    "\n",
    "logdir = log_dir(\"lasso\")\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "checkpoint_path = \"/tmp/my_lasso_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./my_lasso_model\"\n",
    "\n",
    "with tf.name_scope(\"lasso_regression\"):\n",
    "    with tf.name_scope(\"model\"):\n",
    "        X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "        y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "        theta = tf.Variable(tf.random_normal(shape=(n+1,1), seed=42, dtype=tf.float32), name=\"theta\")\n",
    "        y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "    with tf.name_scope(\"train\"):\n",
    "        error = y_pred - y\n",
    "        reg_parm = tf.multiply(alpha,theta)\n",
    "        cost = tf.add(tf.reduce_mean(tf.square(error)), reg_parm, name=\"cost\")\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        training_op = optimizer.minimize(cost)\n",
    "    with tf.name_scope(\"init\"):\n",
    "        init = tf.global_variables_initializer()\n",
    "    with tf.name_scope(\"save\"):\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        # if the checkpoint file exists, restore the model and load the epoch number\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"Cost =\", cost.eval())\n",
    "            sess.run([training_op])\n",
    "        \n",
    "        if epoch % 500 == 0:\n",
    "            saver.save(sess, checkpoint_path)\n",
    "            with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "                f.write(b\"%d\" % (epoch + 1))\n",
    "\n",
    "    saver.save(sess, final_model_path)\n",
    "    os.remove(checkpoint_epoch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
