{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Job Interview Assignment",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNKIMlEDZ_Vw",
        "colab_type": "text"
      },
      "source": [
        "# Job Interview Assignment Submission\n",
        "## by Craig Barbisan\n",
        "\n",
        "**Problem Description:**\n",
        "For every transaction ID, find the last price published prior or equal to the time of that transaction. The output should be in CSV format, where each row stores the transaction ID, the price of that transaction, along with time that the transaction was executed on.\n",
        "\n",
        "**Additional Notes:**\n",
        "* The published and purchase times of each currency price and transaction are unrelated. If a transaction is executed at time ‘x’, then the price of that transaction is published at time ‘x – k’, where k can be zero or\n",
        "any other number of units of time. Don’t try to match the transaction time with an exact published time.\n",
        "* Since we work with Google Cloud and Apache Beam to process large streaming data, implementing the solution in Apache Beam or Spark is preferred. However feel free to use anything else which you feel is\n",
        "best.\n",
        "\n",
        "**Data Files:**\n",
        "\n",
        "2 data files, in .csv format, were provided as inputs for this assignment:\n",
        "\n",
        "1.   prices-2019-02-13.csv - Contains a list of the prices of an unnamed currency throughout the course of the day on Feb 13, 2019. The file includes these fields in the following order:\n",
        "\n",
        ">> CURRENCY_PRICE - The price of the currency<br/>\n",
        "> PUBLISHED_TIME - The date/time at which CURRENCY_PRICE was first effective.\n",
        "\n",
        "2.   transactions-2019-02-13.csv - Contains a list of currency purchase transactions, throughout the course of the day on Feb 13, 2019. The file includes these fields in the following order:\n",
        "\n",
        ">> TRANSACTION_ID - A numeric identifier for a currency purchase transaction.<br/>\n",
        "> TRANSACTION_TIME - The date/time at which the transaction occurred.\n",
        "\n",
        "**Data Profiling**\n",
        "\n",
        "The following is a list of findings from data profiling prior to solution design:\n",
        "\n",
        "* TRANSACTION_ID is NOT a unique identifier in the transaction file.\n",
        "* The combination of TRANSACTION_ID and TRANSACTION_TIME is unique. Thus we can lookup different prices for the same TRANSACTION_ID, depending on when the transaction occurred.\n",
        "* PUBLISHED_TIME for prices IS unique, so we should never find more than 1 price to apply to a transaction.\n",
        "* All timezones in both TRANSACTION_TIME and PUBLISHED_TIME are UTC.\n",
        "* Both the transaction data and price data span multiple days (Feb 13 and Feb 14)\n",
        "* For the entire range of PUBLISHED_TIME values, there is a price in the file for every second in that range.\n",
        "* PUBLISHED_TIME and TRANSACTION_TIME are measured down to the nanosecond.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPIHwglOPEtQ",
        "colab_type": "text"
      },
      "source": [
        "# Solution Design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX4XTSJ_P2tR",
        "colab_type": "text"
      },
      "source": [
        "##Approach\n",
        "\n",
        "1. Bounded PCollections will be used for the input data, 1 each for prices and transactions.\n",
        "2. Parse all elements of each PCollection into tuples of typed values.\n",
        "3. Index each price tuple with its PUBLISHED_TIME value in format YYYYMMDDHHMISS. This implies that for any given second of the day, this collection will contain multiple price tuples.\n",
        "4. Create a dictionary view of the indexed price collection so it can be used as a side-input. This provides the capability to lookup currency prices that were published within a particular second, and will significantly reduce the number of elements to be processed when searching for a price.\n",
        "5. For each transaction, lookup all prices for the second when the transaction occurred, and the previous second. The previous second is required for the scenario where the transaction occurred in second x, prior to the first price published in second x. In this case, we pickup the last price published in second x-1.\n",
        "6. Add the looked-up price to the transaction tuple.\n",
        "7. Format the transaction tuples and output to a file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFT0WxeQPTy6",
        "colab_type": "text"
      },
      "source": [
        "##Pipeline Data Flow\n",
        "![](https://s3pzxq.bn.files.1drv.com/y4m3LMTOQxIEcrNc0_GFZ_Zk9kMNx-1l-20tzh5TElTFl0250yMcAHpyB7x5uSxuFbGf6uWGxgXNBYIcfB5n5vb280vqGNTbxKbjIQky_1U_Vz9XmzeqVXqrqmAg77152YUTsajhnA99-KTm1fiSKbam6SYz7U899KMXdp5Zj4_-Qkmc7JpOcCt8Q1i-PpoNcHA7zT6IjLogiZIxib2H7jGhg?width=660&height=643&cropmode=none)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teTVQ_usAQXe",
        "colab_type": "text"
      },
      "source": [
        "#Assumptions\n",
        "\n",
        "1. Since all timestamps are captured in UTC, there is no requirement for translating date/time values across timezones.\n",
        "\n",
        "2. Since TRANSACTION_ID is not a unique identifier, then multiple prices should be assigned to the same transaction ID (though only 1 price assigned per record).\n",
        "\n",
        "3. There will be at least 1 price record in the price file, for every second within the range of date/time values in the transaction file. This assumption seemed safe, since in the test file, there was typically between 20-40 prices within any given second."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz6KSQ13_3Rr",
        "colab_type": "text"
      },
      "source": [
        "# Setup\n",
        "\n",
        "This cell contains code for environment setup, which includes:\n",
        "1. Installing required python packages.\n",
        "2. Creating a folder to store data files.\n",
        "3. Setting up file names.\n",
        "4. Downloading input files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOOk81Jj_yUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run a shell command.\n",
        "def run(cmd):\n",
        "    print('>> {}'.format(cmd))\n",
        "    !{cmd}\n",
        "    print('')\n",
        "\n",
        "# Install dependencies\n",
        "run('pip -q install apache-beam[interactive]')\n",
        "run('pip -q install attotime')\n",
        "\n",
        "# Create local folder to contain data files\n",
        "run('mkdir -p data')\n",
        "\n",
        "# Data file names\n",
        "txn_filename = './data/transactions-2019-02-13.csv'\n",
        "price_filename = './data/prices-2019-02-13.csv'\n",
        "output_filename = './data/out'\n",
        "\n",
        "# Download the input files into the local file system.\n",
        "import os\n",
        "import urllib.request as rq\n",
        "rq.urlretrieve('https://cbarbisan.netlify.com/prices-2019-02-13.csv', price_filename)\n",
        "rq.urlretrieve('https://cbarbisan.netlify.com/transactions-2019-02-13.csv', txn_filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQCkrxRyQc4I",
        "colab_type": "text"
      },
      "source": [
        "# Helper Functions and Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTiOx6l7Qg85",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import apache_beam as beam\n",
        "from attotime import attodatetime, attotimedelta\n",
        "\n",
        "# For converting the data file representation of a date/time value\n",
        "# to a datetime object which supports nanoseconds.\n",
        "def to_datetime(d):\n",
        "    return attodatetime(year=int(d[0:4]), \\\n",
        "        month=int(d[5:7]), \\\n",
        "        day=int(d[8:10]), \\\n",
        "        hour=int(d[11:13]), \\\n",
        "        minute=int(d[14:16]), \\\n",
        "        second=int(d[17:19]), \\\n",
        "        microsecond=int(d[20:26]), \\\n",
        "        nanosecond=int(d[26:29]) \\\n",
        "  )\n",
        "\n",
        "# Splits the record into separate typed fields in a tuple\n",
        "def parse_record(line):\n",
        "    rec = line.split(',')\n",
        "    dt = to_datetime(rec[1])\n",
        "    return (float(rec[0]), dt)\n",
        "\n",
        "# Creates an index value using a portion of the datetime\n",
        "# argument formatted as YYYYMMDDHHMISS\n",
        "def create_index(dt):\n",
        "    return (dt.strftime('%Y%m%d%H%M%S'))\n",
        "\n",
        "# Creates a tuple whose elements are:\n",
        "#   1. index value\n",
        "#   2. price record tuple\n",
        "def index_price(p):\n",
        "    return(create_index(p[1]), p)\n",
        "\n",
        "# DoFn for a ParDo transform which gets applied to elements in the\n",
        "# transaction PCollection.\n",
        "# Uses a portion of the TRANSACTION_TIME value to lookup all price tuples\n",
        "# which were published in the same second as the transaction, and the second\n",
        "# prior. Searches through those tuples to find the last price published before\n",
        "# the transaction and includes it in the transaction tuple.\n",
        "class EnhanceTransaction(beam.DoFn):\n",
        "    def process(self, element, lkp):\n",
        "        txn_time = element[1]\n",
        "        lkp_key = create_index(txn_time)\n",
        "        lkp_prev_key = create_index(txn_time - attotimedelta(0,1))\n",
        "\n",
        "        prev_list = lkp.get(lkp_prev_key)\n",
        "        if (prev_list == None):\n",
        "            listPrices = lkp.get(lkp_key)\n",
        "        else:\n",
        "            listPrices = prev_list + lkp.get(lkp_key)\n",
        "    \n",
        "        # Need a loop that finds the price that was published immediately\n",
        "        # before txnTime\n",
        "        # price is a tuple of price and published time\n",
        "        published_price = 0\n",
        "        prev_published_time = to_datetime('1970-01-01 00:00:00.000000000 UTC')\n",
        "        for price in listPrices:\n",
        "            published_time = price[1]\n",
        "            if (txn_time >= published_time) \\\n",
        "                and (published_time > prev_published_time):\n",
        "                    published_price = price[0]\n",
        "            prev_published_time = published_time\n",
        "\n",
        "        yield element + (published_price,)\n",
        "\n",
        "# Formats the transaction tuple to match the output file requirements.\n",
        "def format_output(e):\n",
        "    return ','.join((str(int(e[0])), str(e[2]), e[1].strftime('%Y-%m-%d %H:%M:%S.') \\\n",
        "            + str(e[1].microsecond) + str(e[1].nanosecond) + ' UTC'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPvvFB19uXNw",
        "colab_type": "text"
      },
      "source": [
        "# Data Pipeline\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUqfqWyMuIfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Running locally in the DirectRunner.\n",
        "with beam.Pipeline() as pipeline:\n",
        "\n",
        "    pcPrice = (\n",
        "        pipeline | 'Read price records' >> beam.io.ReadFromText(price_filename, skip_header_lines=1)\n",
        "        | 'Parse price records' >> beam.Map(parse_record)\n",
        "        | 'Index price' >> beam.Map(index_price)\n",
        "        | 'Group by key' >> beam.GroupByKey()\n",
        "    )\n",
        "\n",
        "    vPrice = beam.pvalue.AsDict(pcPrice)\n",
        "\n",
        "    pcTxn = (\n",
        "        pipeline | 'Read txn records' >> beam.io.ReadFromText(txn_filename, skip_header_lines=1, strip_trailing_newlines=True)\n",
        "        | 'Parse txn records' >> beam.Map(parse_record)\n",
        "        | 'Enhance transaction' >> beam.ParDo(EnhanceTransaction(), vPrice)\n",
        "        | 'Format output' >> beam.Map(format_output)\n",
        "        | beam.io.WriteToText(output_filename, file_name_suffix='.csv', \\\n",
        "               header='TRANSACTION_ID,CURRENCY_PRICE,TRANSACTION_TIME')\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}